{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [What is Statistical Learning?](#whatis)\n",
    "   - [Why Estimate $f$ ?](#why-estimate-f)\n",
    "   - [How Do We Estimate $f$ ?](#how-estimate-f)\n",
    "   - [Accuracy & Interpretability Tradeoff](#tradeoff-acc-interpr)\n",
    "   - [Supervised Versus Unsupervised Learning](#supervised-unsupervised)\n",
    "   - [Regression Versus Classification](#regression-classification)\n",
    "   \n",
    "\n",
    "### [Assessing Model Accuracy](#model-accuracy)\n",
    "   - [Measuring the Quality of Fit](#quality-of-fit)\n",
    "   - [The Bias-Variance Trade-Off](#bias-variance-tradeoff)\n",
    "   - [The Classification Setting](#classification-setting)\n",
    "      - [The Bayes Classifier](#bayes-classifier)\n",
    "      - [K-Nearest Neighbors](#knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"whatis\"></a>\n",
    "# What is Statistical Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modo corretto di modellare la relazione fra una **response** $Y$ e un **predictor** $X$ è il seguente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large Y=f(X)+\\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione $f$ è la **systematic information** che $X$ fornisce riguardo $Y$.\n",
    "\n",
    "Il termine $\\epsilon$ prende il nome di **error term**, e rappresenta una variabile aleatoria indipendente da $X$ e con media nulla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando si parla di *statistical learning* si intende l'insieme di tecniche utilizzabili per stimare una certa *systematic information* a partire dai dati. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-estimate-f\"></a>\n",
    "## Why Estimate $f$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La stima della *systematic information* è utile a due obiettivi: *prediction* e *inference*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso la stima della *systematic information* $\\hat{f}$ è trattata come una **black box**: non è importante conoscere la sua esatta struttura interna, bensì che fornisca predizioni accurate per $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuratezza di $\\hat{Y}=\\hat{f}(X)$ dipende da due quantità: il **reducible error** e l'**irreducible error**.\n",
    "\n",
    "In generale, $\\hat{f}$ non sarà una stima perfetta di $f$, e ciò introdurrà degli errori nelle previsioni della *response*. Tali errori, però, sono **riducibili** perché possiamo utilizzare delle tecniche di *statistical learning* più appropriate per stimare $f$.\n",
    "\n",
    "Anche se fossimo in grado di stimare $f$ perfettamente, le previsioni del modello conterrebbero comunque degli errori. Questo perché $Y$ è anche funzione dell'*error term* $\\epsilon$, che per definizione non può essere predetto tramite $X$. Di conseguenza, la variabilità associata all'*error term* è in grado di influenzare l'accuratezza delle previsioni del modello. Questo tipo di errore è **irriducibile**: a prescindere dalla bontà della stima di $f$, non possiamo ridurre l'errore introdotto da $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'*error term* $\\epsilon$ può essere dovuto a delle variabili **non misurate** e quindi non incluse nei *predictors* $X$, ma anche a delle variabili **non misurabili**, come il periodo temporale in cui è effettuata la misurazione, o lo stato degli strumenti utilizzati o, più in generale, l'intero contesto operativo dell'esperimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' stato dimostrato che l'**expected value** della differenza quadrata fra la *response* e la *prediction* fornita dal modello è funzione di entrambi i tipi di errore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large E\\left(Y-\\hat{Y}\\right)^2=\n",
    "\\left[f(X)-\\hat{f}(X)\\right]^2+Var(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **reducible error** è definito matematicamente come la differenza fra la *systematic information* e la nostra stima, mentre l'**irreducible error** come la **varianza** della variabile aleatoria $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso la stima $\\hat{f}$ non può essere trattata come una *black box*, bensì è necessario conoscerne l'esatta struttura interna. L'obiettivo non è tanto costruire delle previsioni, quanto approfondire la relazione tra *predictors* e *response*.\n",
    "\n",
    "Domande tipiche dell'obiettivo inferenziale sono: _Quali predittori sono associati alla response? Qual è la relazione esistente tra la response e ciascun predittore? La relazione fra $Y$ e ciascun predittore può essere modellata con una funzione lineare o si necessita di modelli più complessi?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-estimate-f\"></a>\n",
    "## How Do We Estimate $f$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nostro obiettivo è applicare un *learning method* ai dati di *training* per stimare la funzione sconosciuta $f$, cioè la *systematic information*. In altre parole, vogliamo costruire una funzione $\\hat{f}$ tale per cui, per ogni osservazione di *training* $(x,y)$ si abbia $y\\approx \\hat{f}(x)$.\n",
    "\n",
    "La maggior parte dei *learning method* ricadono in una delle due seguenti categorie: *parametric* o *non-parametric*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa è necessario compiere un'assunzione sull'andamento generale della *systematic information* $f$. Ad esempio, se supponiamo che $f$ sia una funzione lineare, il problema di *estimation* si semplifica enormemente: al posto di stimare un'intera funzione arbitraria $p$-dimensionale, è necessario stimare solo $p+1$ coefficienti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large f(X)=\\beta_0+\\beta_1X_1+\\beta_2X_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiere un'assunzione sull'andamento di $f$ vuol dire sostanzialmente scegliere un *modello matematico* che la approssimi, come quello lineare appena descritto.\n",
    "\n",
    "Dopo aver scelto un modello, ci serve una procedura per addestrarlo: in questo caso, la procedura di addestramento non fa altro che sfruttare i dati a disposizione per produrre delle stime dei coefficienti $\\beta_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un potenziale svantaggio dei metodi parametrici è che se il modello matematico scelto non riflette la vera forma di $f$, le previsioni prodotte saranno poco accurate.\n",
    "\n",
    "Potremmo risolvere il problema scegliendo dei modelli più flessibili, capaci di *fittare* tanti andamenti differenti per $f$, ma ciò porterebbe ad un aumento del numero di parametri da stimare. Questi modelli più complessi possono portare il modello in **overfitting**, costringendolo a seguire il *rumore* dei dati con troppa precisione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I metodi non parametrici non si basano su assunzioni esplicite riguardo l'andamento generale di $f$, bensì cercano produrre una stima che sia più vicina possibile ai *data points* a disposizione.\n",
    "\n",
    "Evitando di compiere assunzioni a priori, i metodi non parametrici hanno la possibilità di *fittare* un insieme maggiore di possibili andamenti per $f$, quindi non corrono gli stessi rischi dei metodi parametrici. Hanno però uno svantaggio: non riducendo il problema di *estimation* al calcolo di un piccolo numero di parametri, servono molte più osservazioni di *training* per produrre una stima accurata di $f$.\n",
    "\n",
    "Inoltre, i modelli generati da metodi non parametrici sono generalmente associati a variabilità maggiori, ed è molto più probabile che vadano in **overfitting**, producendo così delle previsioni meno accurate su osservazioni che non hanno fatto parte del processo di *training*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tradeoff-acc-interpr\"></a>\n",
    "## Accuracy & Interpretability Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perché dovremmo scegliere un metodo più restrittivo rispetto ad uno più flessibile? Se il nostro obiettivo è l'inferenza, ad esempio, i modelli più rigidi risultano essere molto più interpretabili, ed è più semplice comprendere la relazione fra *response* e *predictors*. Al contrario, gli approcci più flessibili possono produrre delle stime di $f$ così complicate che risulta difficile intuire in che modo ciascun predittore influenzi la *response*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono casi, invece, in cui l'obiettivo principale è la qualità predittiva, e l'interpretabilità del modello non è importante. In situazioni del genere sarebbe logico usufruire dei *learning method* più flessibili e complessi, ma non è sempre così: spesso si ottengono *predictions* più accurate utilizzando modelli meno flessibili, che oltre a fornire più informazioni sull'associazione *predictor/response*, sono anche molto meno inclini all'**overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"supervised-unsupervised\"></a>\n",
    "## Supervised Versus Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La maggior parte dei problemi di *statistical learning* ricadono in una delle due seguenti categorie: *supervised* o *unsupervised*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nei task **supervised**, ad ogni misurazione dell'insieme dei predittori $x_i$ è associata una misurazione della *response* $y_i$.\n",
    "\n",
    "L'obiettivo è addestrare un modello che associ la *response* ai *predictors*, con l'obiettivo di predire accuratamente la *response* per osservazioni future (*prediction*) o approfondire la relazione fra *response* e *predictors* (*inference*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nei task **unsupervised**, invece, ad ogni misurazione dell'insieme dei predittori $x_i$ non è associata alcuna *response* $y_i$. Il nome deriva proprio dal fatto che ci manca una variabile che *supervisioni* la nostra analisi.\n",
    "\n",
    "L'obiettivo, in questo caso, è approfondire la relazione esistente tra i predittori o fra le osservazioni che compongono il dataset a disposizione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono anche situazioni ibride: supponiamo che, delle $n$ osservazioni di *training*, solo $m$ di esse presentino una misurazione di *response*; per le restanti $n-m$ osservazioni abbiamo a disposizione solo i valori dei *predictors*. In genere ciò accade quando il processo di misurazione della *response* è molto costoso, mentre quello dei *predictors* è più economico.\n",
    "\n",
    "Questi scenari descrivono problemi di **semi-supervised** *learning*, in cui l'obiettivo è usare un *learning method* capace di incorporare sia le osservazioni dotate di *response* sia quelle per cui conosciamo solo i predittori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"regression-classification\"></a>\n",
    "## Regression Versus Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le variabili trattate nei problemi di *statistical learning* possono essere di due tipi: **quantitative** o **qualitative**.\n",
    "\n",
    "I problemi in cui la *response* è quantitativa sono chiamati **regression** *problems*, mentre quelli in cui la *response* è qualitativa sono chiamati **classification** *problems*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-accuracy\"></a>\n",
    "# Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il teorema del **no free lunch** afferma che nessun *learning method* domina sugli altri per tutti i possibili dataset. In altre parole, dato un certo dataset, è sicuramente possibile individuare il metodo che risulta più efficace degli altri, ma è altamente probabile che lo stesso metodo non sarà così accurato su dataset differenti, per i quali saranno altri i metodi che forniranno prestazioni superiori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quality-of-fit\"></a>\n",
    "## Measuring the Quality of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per valutare le performance di un *learning method*, dobbiamo quantificare  il grado di corrispondenza delle *predicted responses* fornite dal modello rispetto alle *true responses* dei dati che abbiamo a disposizione.\n",
    "\n",
    "Nei problemi di regressione, la metrica più utilizzata a questo scopo è il **mean squared error**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large MSE=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\hat{f}(x_i)\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se il *MSE* è calcolato a partire dai dati di *training*, si parla di **training MSE**. In realtà non siamo particolarmente interessati a quanto bene il modello operi sui dati di *training*, bensì ci interessa l'accuratezza delle previsioni ottenute su osservazioni che non hanno preso parte al processo di addestramento, cioè i dati di *test*.\n",
    "\n",
    "In altre parole, vogliamo scegliere il modello che presenti il più basso **test MSE**, e non il più basso *training MSE*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/training-test-mse.PNG\" width=300 height=200></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel grafico precedente si evidenzia l'andamento del *training MSE* (in grigio) e del *test MSE* (in rosso) in funzione del grado di flessibilità/complessità del modello utilizzato. La linea tratteggiata orizzontale rappresenta l'*irreducible error*, cioè $Var(\\epsilon)$, che coincide con il valore minimo che il *test MSE* è in grado di raggiungere a prescindere dal *learning method* utilizzato.\n",
    "\n",
    "Notiamo come, all'aumentare della flessibilità, il *training MSE* decresca continuamente, in riferimento alla maggiore capacità dei modelli più complessi di aderire ai dati di *training* e di produrre *predicted responses* molto vicine alle *real responses*.\n",
    "\n",
    "Il *test MSE*, invece, tende a decrescere per determinati valori di flessibilità, ma ad un certo punto torna ad incrementare il proprio valore. Questa **U-Shape** descritta dal *test MSE* è una proprietà fondamentale del *machine learning*, ed è valida a prescindere dal dataset in uso e dal *learning method* applicato.\n",
    "\n",
    "Quando un *learning method* presenta basso *training MSE* ed alto *test MSE*, si dice che sta **overfittando** i dati. Ciò si verifica perché il modello sta cercando in tutti i modi di trovare dei *pattern* nei dati, e finisce per apprendere *pattern* dovuti a semplice rumore o casualità e non a reali proprietà della *systematic information*. Questi *pattern* non sono presenti nei dati di *test*, ed è per questo motivo che il *test MSE* risulta particolarmente alto.\n",
    "\n",
    "In generale, a prescindere dal possibile *overfitting*, ci aspettiamo quasi sempre che il *training MSE* sia inferiore rispetto al *test MSE*, poiché la maggior parte dei *learning method* sono programmati proprio per minimizzare l'errore sui dati di *training*. L'**overfitting**, quindi, va dichiarato nel momento in cui un modello meno flessibile/complesso di quello utilizzato sarebbe in grado di fornire un *test MSE* inferiore di quello appena prodotto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-variance-tradeoff\"></a>\n",
    "## The Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **variance** di un *learning method* fa riferimento all'entità della variazione di $\\hat{f}$ in risposta ad una variazione dei dati di *training*. In genere, usando diversi *training set* dovremmo ottenere stime $\\hat{f}$ diverse tra loro, ma idealmente l'entità della differenza fra queste stime dovrebbe essere limitata. Se un *learning method* è caratterizzato da alta varianza, bastano delle piccole modifiche al *training set* per provocare variazioni importanti nella corrispondente stima $\\hat{f}$.\n",
    "\n",
    "In media, i *learning method* più flessibili e più complessi presentano valori di *variance* maggiori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **bias** di un *learning method* fa riferimento all'errore introdotto nella stima $\\hat{f}$ dovuto all'approssimazione di una *systematic information* particolarmente complessa con un modello estremamente semplice. Ad esempio, se la vera $f$ ha un andamento non lineare, non sarà mai possibile produrre una stima accurata con un modello lineare. Si può affermare, quindi, che per un task del genere la regressione lineare presenta un alto valore di *bias*.\n",
    "\n",
    "In media, i *learning method* più flessibili e più complessi presentano *bias* inferiori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile dimostrare che, data un'osservazione di test $x_0$, il corrispondente *expected test MSE* può essere decomposto nella somma di tre quantità fondamentali: la **varianza** di $\\hat{f}(x_0)$, il quadrato del **bias** di $\\hat{f}(x_0)$ e la varianza dell'*error term* $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large E\\left(y_0-\\hat{f}(x_0)\\right)^2=\n",
    "Var(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'*expected test MSE* rappresenta il *MSE* medio che otterremmo se stimassimo $f$ usando un gran numero di *training sets* e testassimo ogni $\\hat{f}$ su $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'equazione appena descritta è la modellazione matematica del **Bias-Variance Trade-Off**.\n",
    "\n",
    "Possiamo affermare, quindi, che per minimizzare l'errore sui dati di test dobbiamo scegliere un *learning method* che presenti contemporaneamente bassa varianza e basso bias. Notiamo, inoltre, che il *test MSE* non può mai scendere al di sotto del valore limite $Var(\\epsilon)$, cioè l'*irreducible error*.\n",
    "\n",
    "In linea generale, all'aumentare della flessibilità/complessità del modello, la varianza aumenta ed il bias diminuisce. La velocità con cui le due quantità si modificano determina l'incremento o il decremento del *test MSE*.\n",
    "\n",
    "All'atto pratico, in corrispondenza dei primi aumenti di flessibilità, il decremento di bias è superiore rispetto all'incremento di varianza, portando il *test MSE* a decrescere. Ad un certo punto, però, l'aumento di flessibilità non ha più impatto sul bias, e spinge la varianza a crescere esponenzialmente, portando così il anche il *test MSE* a crescere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/bias-variance-tradeoff.PNG\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel grafico precedente notiamo l'andamento di *bias*, *varianza* e *test MSE* per tre problemi diversi. Notiamo che la flessibilità ottimale, cioè quel grado di complessità del modello che fornisce il miglior valore di *test MSE*, è diverso per ogni task, ma l'andamento generale delle tre quantità rispetta sempre il *bias-variance trade-off*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordiamo, infine, che nei task reali in cui la *systematic information* è ignota, è impossibile calcolare esplicitamente il *bias* o la *varianza* per i diversi *learning method* esistenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classification-setting\"></a>\n",
    "## The Classification Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metrica più comune per quantificare l'accuratezza della stima $\\hat{f}$ in task di classificatione è l'**error rate**, definito come la proporzione di classificazioni non corrette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\text{Error Rate}=\\frac{1}{n}\\sum_{i=1}^{n}I(y_i,\\hat{y}_i)\n",
    "\\;\\;\\;\\begin{cases}\n",
    "I(y_i,\\hat{y}_i)=1 & \\text{if}\\;\\;\\; y_i=\\hat{y}_i\n",
    "\\\\ I(y_i,\\hat{y}_i)=0 & \\text{if}\\;\\;\\; y_i=\\hat{y}_i\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come nel caso dei task di regressione, siamo interessati al valore che l'*error rate* assume su dati non usati nel processo di addestramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes-classifier\"></a>\n",
    "### Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile dimostrare che il *test error rate* di un problema di classificazione è minimizzato, in media, da un modello che opera assegnando a ciascuna osservazione la sua classe più probabile, dati i valori dei suoi *predictors*.\n",
    "\n",
    "In altre parole, supponendo di avere un solo *predictor* $X$, all'osservazione di test $x_0$ è assegnata la classe per la quale la seguente probabilità condizionata è massimizzata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large P(Y=j|X=x_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo semplice classificatore prende il nome di **Bayes Classifier**.\n",
    "\n",
    "In un problema di classificazione binaria, il *bayes classifier* assegna all'osservazione di test $x_0$ la classe positiva nel caso in cui $P(Y=1|X=x_0)>0.5$, e la classe negativa nel caso in cui $P(Y=1|X=x_0)\\leq0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/bayes-decision-boundary.PNG\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel precedente grafico è mostrata l'applicazione del *bayes classifier* su un dataset simulato con due predittori.\n",
    "\n",
    "La regione arancione del *feature space* rappresenta l'insieme dei valori dei predittori per cui $P(Y=\\text{orange}|X_1,X_2)>0.5$, mentre la regione *viola* rappresenta l'insieme dei valori dei predittori per cui $P(Y=\\text{orange}|X_1,X_2)\\leq0.5$.\n",
    "\n",
    "La linea tratteggiata evidenzia l'insieme dei valori dei predittori per cui la probabilità condizionata è esattamente uguale a $0.5$, e prende il nome di **Bayes Decision Boundary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifichiamo che il *bayes classifier* può essere utilizzato solo perché i dati utilizzati sono simulati, pertanto le probabilità condizionate rispetto ai valori dei predittori sono calcolabili a priori con precisione.\n",
    "\n",
    "Inoltre, nonostante il *bayes classifier* sia il più accurato fra i *learning method* di classificazione, non è detto che il suo *error rate* sia sempre nullo. Ciò è dovuto al fatto che i *data points* possono risultare come sovrapposti fra di loro, assumendo valori di *predictors* molto simili o coincidenti e creando delle ambiguità nel calcolo delle probabilità condizionate.\n",
    "\n",
    "E' stato dimostrato che il **Bayes Error Rate** è analogo alla varianza dell'*error term* $\\epsilon$, cioè coincide con l'*irriducible error* $Var(\\epsilon)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per risolvere un task di classificazione, l'ideale sarebbe usare sempre il *bayes classifier*, ma non conoscendo la distribuzione condizionata di $Y$ dato $X$ a livello di popolazione, ciò risulta impossibile.\n",
    "\n",
    "Molti *learning method*, però, si ispirano al *bayes classifier* e provano a stimare la distribuzione condizionata di $Y$ dato $X$, assegnando ad ogni osservazione la classe a cui corrisponde la maggiore probabilità *stimata*. Uno di questi metodi è il **K-Nearest Neighbors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato un intero positivo $K$ e un'osservazione di test $x_0$, il *KNN* procede in tre step:\n",
    "- Identifica i $K$ *data point* nel *training set* che risultano più vicini a $x_0$, denotandoli con $N_0$.\n",
    "\n",
    "\n",
    "- Stima la probabilità condizionata che l'osservazione di test appartenga alla generica classe $j$ come la proporzione di *data points* presenti in $N_0$ la cui *response* coincide con $j$.\n",
    "\n",
    "\n",
    "- Assegna all'osservazione di test la classe a cui corrisponde la maggiore probabilità condizionata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in N_0}I(y_i=j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonostante la sua semplicità, il *KNN* è capace di produrre classificatori molto simili al *bayes classifier*.\n",
    "\n",
    "Un ruolo importante è svolto dall'iperparametro $K$. Con $K=1$, il *decision boundary* è estremamente flessibile ed è influenzato da pattern nei dati che non sono presenti nel *bayes decision boundary*; questo descrive un classificatore con *low bias* e *high variance*. Man mano che $K$ cresce, il modello diventa sempre meno flessibile, e produce *decision boundary* dall'andamento sempre più lineare; questo descrive un classificatore con *low variance* e *high bias*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/knn-decision-boundary.PNG\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se plottiamo l'andamento del *training error* e del *test error* in funzione di $\\frac{1}{K}$, notiamo il classico andamento *U-shaped* del *test error* e una decrescita continua del *training error*.\n",
    "\n",
    "Ricordiamo che, se all'aumentare di $K$ il modello diventa sempre meno flessibile, plottare gli errori in funzione di $1/K$ vuol dire spostarsi da modelli poco flessibili ($K$ alto) a modelli sempre più flessibili ($K$ basso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/knn-test-error.PNG\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
